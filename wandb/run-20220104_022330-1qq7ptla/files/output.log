[32m[I 2022-01-04 02:23:43,935][39m A new study created in memory with name: my_study
  0%|          | 0/5 [00:00<?, ?it/s]/home/puppy/projects/BCI/models/loss.py:89: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.
L = torch.cholesky(A)
should be replaced with
L = torch.linalg.cholesky(A)
and
U = torch.cholesky(A, upper=True)
should be replaced with
U = torch.linalg.cholesky(A).transpose(-2, -1).conj().
This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180487213/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:1285.)
  L = torch.cholesky(S_t, upper=False)
/home/puppy/projects/BCI/models/loss.py:90: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.
torch.linalg.solve has its arguments reversed and does not return the LU factorization.
To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.
X = torch.solve(B, A).solution
should be replaced with
X = torch.linalg.solve(A, B) (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180487213/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)
  P = torch.solve(S_w, L)[0] @ torch.pinverse(L).t()
/home/puppy/projects/BCI/models/loss.py:92: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.
The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.
L, _ = torch.symeig(A, upper=upper)
should be replaced with
L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')
and
L, V = torch.symeig(A, eigenvectors=True)
should be replaced with
L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180487213/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:2499.)
  eigen_values, eigen_vectors = torch.symeig(P, eigenvectors=True)



100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:08<00:00, 13.67s/it]




100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:00<00:00, 12.03s/it]




100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:04<00:00, 12.89s/it]




100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:02<00:00, 12.42s/it]




100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:05<00:00, 13.19s/it]
[32m[I 2022-01-04 02:29:05,548][39m Trial 0 finished with value: 0.5800000429153442 and parameters: {'filters': 15, 'optim_type_model': 'adam', 'weight_decay_model': 8.226923898948013e-08, 'lr_model': 0.021442495431989415, 'optim_type_clf': 'adam', 'weight_decay_clf': 1.05151297309393e-06, 'lr_clf': 0.0002129991977487159}. Best is trial 0 with value: 0.5800000429153442.
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 0 < 26; dropping {'filters': 15, 'optim_type_model': 'adam', 'weight_decay_model': 8.226923898948013e-08, 'lr_model': 0.021442495431989415, 'optim_type_clf': 'adam', 'weight_decay_clf': 1.05151297309393e-06, 'lr_clf': 0.0002129991977487159, '5foldValidAccuracy': 0.5800000429153442}.




100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:02<00:00, 12.43s/it]




100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:09<00:00, 13.91s/it]


 20%|â–ˆâ–ˆ        | 1/5 [00:26<01:47, 26.90s/it]